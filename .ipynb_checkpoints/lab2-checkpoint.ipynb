{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll explore acquisition of semi-structured data from a web service, browsing the data and then parsing free-text content. \n",
    "\n",
    "The data come from wikipedia which has a very simple REST API. While the focus is on natural language parsing, we'll also explore JSON and Wikipedia's own markup format. \n",
    "\n",
    "We'll be using network access for this assignment, so MAKE SURE YOUR LAPTOP NETWORK IS UP before starting the VM. \n",
    "\n",
    "Once your VM is up and running, you can download this notebook file by clicking on icon at the top right of this page. Create a directory ~/labs/lab4 to hold it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford Parser Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to install some parsing tools. Download the Stanford parser from <a href=\"https://ufl.instructure.com/files/25762281/download?download_frd=1\">here</a>. If your network is not working, download from a browser on your host machine and then use drag-and-drop.\n",
    "\n",
    "Either way, you can put the parser in your \"Downloads\" directory. Unpack it with\n",
    "<pre>\n",
    "tar xvzf stanfordparser.tar.gz\n",
    "</pre>\n",
    "\n",
    "and then move it to the /opt directory with \n",
    "<pre>\n",
    "sudo mv StanfordParser /opt\n",
    "</pre>\n",
    "\n",
    "It will be helpful to have links to the parser scripts from your bin directory. If you havent already, create a directory ~/bin. Then \n",
    "<pre>\n",
    "cd ~/bin\n",
    "ln -s /opt/StanfordParser/lexparser.sh lexparser.sh\n",
    "ln -s /opt/StanfordParser/lexparser-gui.sh lexparser-gui.sh\n",
    "ln -s /opt/StanfordParser/dependencyviewer/dependencyviewer.sh dependencyviewer.sh\n",
    "</pre>\n",
    "\n",
    "These files will be in your path the next time you login. You can logout from the start button at the top right of the VM window. Then log back in again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mediawiki Parser Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mediawiki parser is memorably named \"mwparserfromhell\". To install it with a working network, all you need to do is\n",
    "<pre>\n",
    "sudo pip install mwparserfromhell\n",
    "</pre>\n",
    "\n",
    "if your network is not working, copy the package source from <a href=\"https://bcourses.berkeley.edu/courses/1267848/files/51008623/download?wrap=1\">here</a>. Untar it, which gives a directory tree starting at \"usr\". Traverse the directories until you find the \"mwparserfromhell\" direcory. Copy that directory to your python modules directory with:\n",
    "<pre>\n",
    "sudo cp -r mwparserfromhell /usr/local/lib/python2.7/dist-packages\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing the Wikipedia Web API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by looking over the <a href=\"http://www.mediawiki.org/wiki/API:Main_page\">Mediawiki API documentation</a> which describes Wikipedia's RESTful API. \n",
    "\n",
    "The code below implements an API call with options:\n",
    "* format=json to receive JSON data\n",
    "* action=query to query Wikipedia content\n",
    "* titles=string to specify a list of page titles to search for\n",
    "* prop=revision to return the revisions of the page\n",
    "* rvprop=content to return the full page content\n",
    "\n",
    "We'll start with the title search string 'parsing' to retrieve the page about parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "title='parsing'\n",
    "response = requests.get(\"http://en.wikipedia.org/w/api.php?format=json&action=query&titles=\"+str(title)+\"&prop=revisions&rvprop=content\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response object is an HTTP GET response. It turns out the requests package contains a json interpreter, which we can invoke as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsondata = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you dont have a working network, copy the file <a href=\"https://bcourses.berkeley.edu/courses/1267848/files/51028371/download?wrap=1\">parsing.json</a> into your ~/labs/lab4 directory. Then you can load it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# fp=open('/home/datascience/labs/lab4/parsing.json','r')\n",
    "# jsondata=json.load(fp);\n",
    "# fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The JSON object is hierarchically structured. To view it, its helpful to define a couple of helper routines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def pretty(jdata):\n",
    "    str = json.dumps(jdata, sort_keys=True, indent=4).decode('string_escape');\n",
    "    return str\n",
    "\n",
    "def saveas(sdata, fname):\n",
    "    f = open(fname,'w');\n",
    "    f.write(sdata);\n",
    "    f.close();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first routine converts the JSON to a carefully-formatted string. The second writes a string to a file. We can use them together to save the JSON data to a better format for viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saveas(pretty(jsondata), '/home/datascience/labs/lab4/'+title+'.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now open the file '/home/datascience/labs/lab4/parsing.json' by right-clicking on it and using \"open-with\" with emacs or gvim. Note the structure.\n",
    "\n",
    "The JSON parser converts JSON data nodes and lists of nodes. The nodes are represented as Python \"Dict\" objects, and the lists are Python lists. Each Dict maps the names of the nodes children to their values. We can query the type of each node using the \"type\" function. For each Dict, we can enumerate the keys using the keys() method. In this way we can explore the JSON tree (although its much quicker to eyeball it from the JSON file we just saved). But anyway we can browse with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(jsondata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'batchcomplete', u'query']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsondata.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is a list of just one string (a unicode string, hence the \"u\" prefix). We can then extract that node with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'normalized': [{u'from': u'parsing', u'to': u'Parsing'}],\n",
       " u'pages': {u'310015': {u'ns': 0,\n",
       "   u'pageid': 310015,\n",
       "   u'revisions': [{u'*': u'{{Redirect|Parse}}\\n{{redirect|Parser|the computer programming language|Parser (CGI language)}}\\n\\n\\'\\'\\'Parsing\\'\\'\\' or \\'\\'\\'syntactic analysis\\'\\'\\' is the process of analysing a [[String (computer science)|string]] of [[Symbol (programming)|symbols]], either in [[natural language]] or in [[computer languages]], conforming to the rules of a [[formal grammar]].  The term \\'\\'parsing\\'\\' comes from Latin \\'\\'pars\\'\\' (\\'\\'orationis\\'\\'), meaning [[Part of speech|part (of speech)]].<ref>{{cite web |url=http://www.bartleby.com/61/33/P0083300.html |title=Bartleby.com homepage |accessdate=28 November 2010}}</ref><ref name=\"dictionary.com\">{{cite web |url=http://dictionary.reference.com/search?q=parse&x=0&y=0 |title=parse |publisher=dictionary.reference.com |accessdate=27 November 2010}}</ref>\\n\\nThe term has slightly different meanings in different branches of [[linguistics]] and [[computer science]].  Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence, sometimes with the aid of devices such as [[sentence diagram]]s.  It usually emphasizes the importance of grammatical divisions such as [[subject (grammar)|subject]] and [[predicate (grammar)|predicate]].\\n\\nWithin [[computational linguistics]] the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a [[parse tree]] showing their syntactic relation to each other, which may also contain semantic and other information.\\n\\nThe term is also used in [[psycholinguistics]] when describing language comprehension.  In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) \"in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc.\"<ref name=\"dictionary.com\" />  This term is especially common when discussing what linguistic cues help speakers to interpret [[garden path sentence|garden-path sentences]].\\n\\nWithin computer science, the term is used in the analysis of [[computer languages]], referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of [[compilers]] and [[interpreter (computing)|interpreters]].\\n\\n== Human languages ==\\n{{main category|Natural language parsing}}\\n\\n===Traditional methods===\\n\\nThe traditional grammatical exercise of parsing, sometimes known as  \\'\\'clause analysis\\'\\', involves breaking down a text into its component parts of speech with an explanation of the form, function, and syntactic relationship of each part.<ref>{{cite web | title=Grammar and Composition | url=http://grammar.about.com/od/pq/g/parsingterm.htm}}</ref> This is determined in large part from study of the language\\'s [[conjugation (grammar)|conjugation]]s and [[declensions]], which can be quite intricate for heavily inflected languages. To parse a phrase such as \\'man bites dog\\' involves noting that the singular noun \\'man\\' is the subject of the sentence, the verb \\'bites\\' is the third person singular of the present tense of the verb \\'to bite\\', and the singular noun \\'dog\\' is the object of the sentence.  Techniques such as [[sentence diagram]]s are sometimes used to indicate relation between elements in the sentence.\\n\\nParsing was formerly central to the teaching of grammar throughout the English-speaking world, and widely regarded as basic to the use and understanding of written language.  However the teaching of such techniques is no longer current.\\n\\n===Computational methods===\\n{{unreferenced section|date=February 2013}}\\n{{Main category|Natural language parsing}}\\n\\nIn some [[machine translation]] and [[natural language processing]] systems, written texts in human languages are parsed by computer programs{{clarify|date=February 2013}}. Human sentences are not easily parsed by programs, as there is substantial [[syntactic ambiguity|ambiguity]] in the structure of human language, whose usage is to convey meaning (or [[semantics]]) amongst a potentially unlimited range of possibilities but only some of which are germane to the particular case. So an utterance \"Man bites dog\" versus \"Dog bites man\" is definite on one detail but in another language might appear as \"Man dog bites\" with a reliance on the larger context to distinguish between those two possibilities, if indeed that difference was of concern. It is difficult to prepare formal rules to describe informal behaviour even though it is clear that some rules are being followed.\\n\\nIn order to parse natural language data, researchers must first agree on the [[grammar]] to be used. The choice of syntax is affected by both [[language|linguistic]] and computational concerns; for instance some parsing systems use [[lexical functional grammar]], but in general, parsing for grammars of this type is known to be [[NP-complete]]. [[Head-driven phrase structure grammar]] is another linguistic formalism which has been popular in the parsing community, but other research efforts have focused on less complex formalisms such as the one used in the Penn [[Treebank]]. [[Shallow parsing]] aims to find only the boundaries of major constituents such as noun phrases. Another popular strategy for avoiding linguistic controversy is [[dependency grammar]] parsing.\\n\\nMost modern parsers are at least partly [[statistics|statistical]]; that is, they rely on a corpus of training data which has already been annotated (parsed by hand). This approach allows the system to gather information about the frequency with which various constructions occur in specific contexts. \\'\\'(See [[machine learning]].)\\'\\' Approaches which have been used include straightforward [[PCFG]]s (probabilistic context-free grammars), [[maximum entropy classifier|maximum entropy]], and [[neural net]]s. Most of the more successful systems use \\'\\'lexical\\'\\' statistics (that is, they consider the identities of the words involved, as well as their [[part of speech]]). However such systems are vulnerable to [[overfitting]] and require some kind of [[smoothing]] to be effective.{{Citation needed|date=May 2008}}\\n\\nParsing algorithms for natural language cannot rely on the grammar having \\'nice\\' properties as with manually designed grammars for programming languages. As mentioned earlier some grammar formalisms are very difficult to parse computationally; in general, even if the desired structure is not [[context-free]], some kind of context-free approximation to the grammar is used to perform a first pass. Algorithms which use context-free grammars often rely on some variant of the [[CYK algorithm]], usually with some [[heuristic (computer science)|heuristic]] to prune away unlikely analyses to save time. \\'\\'(See [[chart parsing]].)\\'\\' However some systems trade speed for accuracy using, e.g., linear-time versions of the [[Shift-reduce parsing|shift-reduce]] algorithm. A somewhat recent development has been [[parse reranking]] in which the parser proposes some large number of analyses, and a more complex system selects the best option.\\n\\n===Psycholinguistics===\\n\\nIn [[psycholinguistics]], parsing involves not just the assignment of words to categories, but the evaluation of the meaning of a sentence according to the rules of syntax drawn by inferences made from each word in the sentence.  This normally occurs as words are being heard or read.  Consequently, psycholinguistic models of parsing are of necessity \\'\\'incremental\\'\\', meaning that they build up an interpretation as the sentence is being processed, which is normally expressed in terms of a partial syntactic structure.  Creation of initially wrong structures occurs when interpreting [[garden path sentences]].\\n\\n== Computer languages ==\\n{{unreferenced section|date=February 2013}}\\n\\n=== Parser ===\\n\\nA \\'\\'\\'parser\\'\\'\\' is a software component that takes input data (frequently text) and builds a [[data structure]] \\u2013 often some kind of [[parse tree]], [[abstract syntax tree]] or other hierarchical structure \\u2013 giving a structural representation of the input, checking for correct syntax in the process. The parsing may be preceded or followed by other steps, or these may be combined into a single step. The parser is often preceded by a separate [[Lexical analysis|lexical analyser]], which creates tokens from the sequence of input characters; alternatively, these can be combined in [[scannerless parsing]]. Parsers may be programmed by hand or may be automatically or semi-automatically generated by a [[parser generator]]. Parsing is complementary to [[templating language|templating]], which produces formatted \\'\\'output.\\'\\' These may be applied to different domains, but often appear together, such as the [[scanf]]/[[printf]] pair, or the input (front end parsing) and output (back end code generation) stages of a compiler.\\n\\nThe input to a parser is often text in some [[computer language]], but may also be text in a natural language or less structured textual data, in which case generally only certain parts of the text are extracted, rather than a parse tree being constructed. Parsers range from very simple functions such as [[scanf]], to complex programs such as the frontend of a [[C++ compiler]] or the [[HTML]] parser of a [[web browser]]. An important class of simple parsing is done using [[regular expression]]s, in which a regular expression defines a [[regular language]] and a regular expression engine automatically generating a parser for that language, allowing pattern matching and extraction of text. In other contexts regular expressions are instead used prior to parsing, as the lexing step whose output is then used by the parser.\\n\\nThe use of parsers varies by input. In the case of data languages, a parser is often found as the file reading facility of a program, such as reading in HTML or [[XML]] text; these examples are [[markup language]]s. In the case of [[programming language]]s, a parser is a component of a [[compiler]] or [[Interpreter (computing)|interpreter]], which parses the [[source code]] of a [[computer programming language]] to create some form of internal representation; the parser is a key step in the [[compiler frontend]]. Programming languages tend to be specified in terms of a [[deterministic context-free grammar]] because fast and efficient parsers can be written for them. For compilers, the parsing itself can be done in one pass or multiple passes \\u2013 see [[one-pass compiler]] and [[multi-pass compiler]].\\n\\nThe implied disadvantages of a one-pass compiler can largely be overcome by adding fix-ups, where provision is made for fix-ups during the forward pass, and the fix-ups are applied backwards when the current program segment has been recognized as having been completed. An example where such a fix-up mechanism would be useful would be a forward GOTO statement, where the target of the GOTO is unknown until the program segment is completed. In this case, the application of the fix-up would be delayed until the target of the GOTO was recognized. Obviously, a backward GOTO does not require a fix-up.\\n\\nContext-free grammars are limited in the extent to which they can express all of the requirements of a language. Informally, the reason is that the memory of such a language is limited. The grammar cannot remember the presence of a construct over an arbitrarily long input; this is necessary for a language in which, for example, a name must be declared before it may be referenced. More powerful grammars that can express this constraint, however, cannot be parsed efficiently. Thus, it is a common strategy to create a relaxed parser for a context-free grammar which accepts a superset of the desired language constructs (that is, it accepts some invalid constructs); later, the unwanted constructs can be filtered out at the [[Semantic analysis (compilers)|semantic analysis]] (contextual analysis) step.\\n\\nFor example, in [[Python (programming language)|Python]] the following is syntactically valid code:\\n<source lang=python>\\nx = 1\\nprint(x)\\n</source>\\nThe following code, however, is syntactically valid in terms of the context-free grammar, yielding a syntax tree with the same structure as the previous, but is syntactically invalid in terms of the context-sensitive grammar, which requires that variables be initialized before use:\\n<source lang=python>\\nx = 1\\nprint(y)\\n</source>\\nRather than being analyzed at the parsing stage, this is caught by checking the \\'\\'values\\'\\' in the syntax tree, hence as part of \\'\\'semantic\\'\\' analysis: context-sensitive syntax is in practice often more easily analyzed as semantics.\\n\\n===Overview of process===\\n[[File:Parser Flow.gif|right|Flow of data in a typical parser]]\\nThe following example demonstrates the common case of parsing a computer language with two levels of grammar: lexical and syntactic.\\n\\nThe first stage is the token generation, or [[lexical analysis]], by which the input character stream is split into meaningful symbols defined by a grammar of [[regular expression]]s. For example, a calculator program would look at an input such as \"<code>12*(3+4)^2</code>\" and split it into the tokens <code>12</code>, <code>*</code>, <code>(</code>, <code>3</code>, <code>+</code>, <code>4</code>, <code>)</code>, <code>^</code>, <code>2</code>, each of which is a meaningful symbol in the context of an arithmetic expression. The lexer would contain rules to tell it that the characters <code>*</code>, <code>+</code>, <code>^</code>, <code>(</code> and <code>)</code> mark the start of a new token, so meaningless tokens like \"<code>12*</code>\" or \"<code>(3</code>\" will not be generated.\\n\\nThe next stage is parsing or syntactic analysis, which is checking that the tokens form an allowable expression. This is usually done with reference to a [[context-free grammar]] which recursively defines components that can make up an expression and the order in which they must appear. However, not all rules defining programming languages can be expressed by context-free grammars alone, for example type validity and proper declaration of identifiers. These rules can be formally expressed with [[attribute grammar]]s.\\n\\nThe final phase is [[Semantic analysis (computer science)|semantic parsing]] or analysis, which is working out the implications of the expression just validated and taking the appropriate action. In the case of a calculator or interpreter, the action is to evaluate the expression or program, a compiler, on the other hand, would generate some kind of code. Attribute grammars can also be used to define these actions.\\n\\n==Types of parsers==\\nThe \\'\\'task\\'\\' of the parser is essentially to determine if and how the input can be derived from the start symbol of the grammar. This can be done in essentially two ways:\\n*[[Top-down parsing]]- Top-down parsing can be viewed as an attempt to find left-most derivations of an input-stream by searching for [[parse tree]]s using a top-down expansion of the given [[formal grammar]] rules. Tokens are consumed from left to right. Inclusive choice is used to accommodate [[ambiguity]] by expanding all alternative right-hand-sides of grammar rules.<ref name=\" AhoSethiUllman 1986\">Aho, A.V., Sethi, R. and Ullman ,J.D.  (1986) \" Compilers: principles, techniques, and tools.\" \\'\\' [[Addison-Wesley Longman]] Publishing Co., Inc. Boston, MA, USA. \\'\\'</ref>\\n*[[Bottom-up parsing]] - A parser can start with the input and attempt to rewrite it to the start symbol. Intuitively, the parser attempts to locate the most basic elements, then the elements containing these, and so on. [[LR parser]]s are examples of bottom-up parsers. Another term used for this type of parser is Shift-Reduce parsing.\\n\\n[[LL parser]]s and [[recursive-descent parser]]  are examples of top-down parsers which cannot accommodate [[left recursion|left recursive]] [[Formal grammar#The syntax of grammars|production rules]]. Although it has been believed that simple implementations of top-down parsing cannot accommodate direct and indirect left-recursion and may require exponential time and space complexity while parsing ambiguous [[context-free grammar]]s, more sophisticated algorithms for top-down parsing have been created by Frost, Hafiz, and Callaghan<ref name=\"FrostHafizCallaghan 2007\">Frost, R., Hafiz, R. and Callaghan, P. (2007) \" Modular and Efficient Top-Down Parsing for Ambiguous Left-Recursive Grammars .\" \\'\\'10th International Workshop on Parsing Technologies (IWPT), ACL-SIGPARSE \\'\\', Pages: 109 - 120, June 2007, Prague.</ref><ref name=\"FrostHafizCallaghan 2008\">Frost, R., Hafiz, R. and Callaghan, P. (2008) \" Parser Combinators for Ambiguous Left-Recursive Grammars.\" \\'\\' 10th International Symposium on Practical Aspects of Declarative Languages (PADL), ACM-SIGPLAN \\'\\', Volume 4902/2008, Pages: 167 - 181, January 2008, San Francisco.</ref> which accommodate [[ambiguity]] and [[left recursion]] in polynomial time and which generate polynomial-size representations of the potentially exponential number of parse trees. Their algorithm is able to produce both left-most and right-most derivations of an input with regard to a given [[context-free grammar]].\\n\\nAn important distinction with regard to parsers is whether a parser generates a \\'\\'leftmost derivation\\'\\' or a \\'\\'rightmost derivation\\'\\' (see [[context-free grammar]]). LL parsers will generate a leftmost [[Parse tree|derivation]] and LR parsers will generate a rightmost derivation (although usually in reverse).<ref name=\" AhoSethiUllman 1986\" />\\n\\n== Types of parsers ==\\n\\n=== Top-down parsers ===\\nSome of the parsers that use [[top-down parsing]] include:\\n* [[Recursive descent parser]]\\n* [[LL parser]] (\\'\\'\\'L\\'\\'\\'eft-to-right, \\'\\'\\'L\\'\\'\\'eftmost derivation)\\n* [[Earley parser]]\\n\\n=== Bottom-up parsers ===\\nSome of the parsers that use [[bottom-up parsing]] include:\\n* Precedence parser\\n** [[Operator-precedence parser]]\\n** [[Simple precedence parser]]\\n* BC (bounded context) parsing\\n* [[LR parser]] (\\'\\'\\'L\\'\\'\\'eft-to-right, \\'\\'\\'R\\'\\'\\'ightmost derivation)\\n** [[SLR parser|Simple LR (SLR) parser]]\\n** [[LALR parser]]\\n** [[Canonical LR parser|Canonical LR (LR(1)) parser]]\\n** [[GLR parser]]\\n* [[CYK algorithm|CYK parser]]\\n* [[Recursive ascent parser]]\\n* [[Shift-Reduce Parser]]\\n\\n===Parser development software===\\nSome of the well known parser development tools include the following.  Also see [[comparison of parser generators]].\\n{{colbegin||20em}}\\n* [[ANTLR]]\\n* [[GNU bison|Bison]]\\n* [[Coco/R]]\\n* [[GOLD (parser)|GOLD]]\\n* [[JavaCC]]\\n* [[Lemon Parser|Lemon]]\\n* [[lex (software)|Lex]]\\n* [[Parboiled (Java)|Parboiled]]\\n* [[Parsec (parser)|Parsec]]\\n* [[Ragel]]\\n* [[Spirit Parser Framework]]\\n* [[Syntax Definition Formalism]]\\n* [[SYNTAX]]\\n* [[XPL]]\\n* [[Yacc]]\\n{{colend}}\\n\\n==Lookahead ==\\n{{unreferenced section|date=April 2012}}\\nLookahead establishes the maximum incoming tokens that a parser can use to decide which rule it should use. Lookahead is especially relevant to [[LL parser|LL]], [[LR parser|LR]], and [[LALR parser]]s, where it is often explicitly indicated by affixing the lookahead to the algorithm name in parentheses, such as LALR(1).\\n\\nMost [[programming language]]s, the primary target of parsers, are carefully defined in such a way that a parser with limited lookahead, typically one, can parse them, because parsers with limited lookahead are often more efficient. One important change{{Citation needed|date=December 2008}} to this trend came in 1990 when [[Terence Parr]] created [[ANTLR]] for his Ph.D. thesis, a [[parser generator]] for efficient LL(\\'\\'k\\'\\') parsers, where \\'\\'k\\'\\' is any fixed value.\\n\\nParsers typically have only a few actions after seeing each token. They are shift (add this token to the stack for later reduction), reduce (pop tokens from the stack and form a syntactic construct), end, error (no known rule applies) or conflict (does not know whether to shift or reduce).\\n\\nLookahead has two advantages.\\n* It helps the parser take the correct action in case of conflicts. For example, parsing the if statement in the case of an else clause.\\n* It eliminates many duplicate states and eases the burden of an extra stack. A C language non-lookahead parser will have around 10,000 states. A lookahead parser will have around 300 states.\\n\\nExample: Parsing the Expression  1 + 2 * 3\\n{| class=\"toccolours\"\\n| colspan=3 | Set of expression parsing rules (called grammar) is as follows,\\n|-\\n| Rule1: || E \\u2192 E + E  || style=\"padding-left:1em\" | Expression is the sum of two expressions.\\n|-\\n| Rule2: || E \\u2192 E * E  || style=\"padding-left:1em\" |Expression is the product of two expressions.\\n|-\\n| Rule3: || E \\u2192 number || style=\"padding-left:1em\" |Expression is a simple number\\n|-\\n| Rule4: || colspan=2 | + has less precedence than *\\n|}\\nMost programming languages (except for a few such as APL and Smalltalk) and algebric formulas give higher precedence to multiplication than addition, in which case the correct interpretation of the example above is (1 + (2*3)).\\nNote that Rule4 above is a semantic rule. It is possible to rewrite the grammar to incorporate this into the syntax. However, not all such rules can be translated into syntax.\\n\\n;Simple non-lookahead parser actions\\n\\nInitially Input = [1,+,2,*,3]\\n# Shift \"1\" onto stack from input (in anticipation of rule3). Input = [+,2,*,3] Stack = [1]\\n# Reduces \"1\" to expression \"E\" based on rule3. Stack = [E]\\n# Shift \"+\" onto stack from input (in anticipation of rule1). Input = [2,*,3] Stack = [E,+]\\n# Shift \"2\" onto stack from input (in anticipation of rule3). Input = [*,3] Stack = [E,+,2]\\n# Reduce stack element \"2\" to Expression \"E\" based on rule3. Stack = [E,+,E]\\n# Reduce stack items [E,+] and new input \"E\" to \"E\" based on rule1. Stack = [E]\\n# Shift \"*\" onto stack from input (in anticipation of rule2). Input = [3] Stack = [E,*]\\n# Shift \"3\" onto stack from input (in anticipation of rule3). Input = [] (empty) Stack = [E,*,3]\\n# Reduce stack element \"3\" to expression \"E\" based on rule3. Stack = [E,*,E]\\n# Reduce stack items [E,*] and new input \"E\" to \"E\" based on rule2. Stack = [E]\\n\\nThe parse tree and resulting code from it is not correct according to language semantics.\\n\\nTo correctly parse without lookahead, there are three solutions:\\n* The user has to enclose expressions within parentheses. This often is not a viable solution.\\n* The parser needs to have more logic to backtrack and retry whenever a rule is violated or not complete. The similar method is followed in LL parsers.\\n* Alternatively, the parser or grammar needs to have extra logic to delay reduction and reduce only when it is absolutely sure which rule to reduce first. This method is used in LR parsers. This correctly parses the expression but with many more states and increased stack depth.\\n\\n;Lookahead parser actions\\n# Shift 1 onto stack on input 1 in anticipation of rule3. It does not reduce immediately.\\n# Reduce stack item 1 to simple Expression on input + based on rule3. The lookahead is +, so we are on path to E +, so we can reduce the stack to E.\\n# Shift + onto stack on input + in anticipation of rule1.\\n# Shift 2 onto stack on input 2 in anticipation of rule3.\\n# Reduce stack item 2 to Expression on input * based on rule3. The lookahead * expects only E before it.\\n# Now stack has E + E and still the input is *. It has two choices now, either to shift based on rule2 or reduction based on rule1. Since * has more precedence than + based on rule4, so shift * onto stack in anticipation of rule2.\\n# Shift 3 onto stack on input 3 in anticipation of rule3.\\n# Reduce stack item 3 to Expression after seeing end of input based on rule3.\\n# Reduce stack items E * E to E based on rule2.\\n# Reduce stack items E + E to E based on rule1.\\nThe parse tree generated is correct and simply more efficient {{Citation needed|date=April 2011}} than non-lookahead parsers. This is the strategy followed in [[LALR parser]]s.\\n\\n== See also ==\\n{{colbegin||22em}}\\n* [[Backtracking]]\\n* [[Chart parser]]\\n* [[Compiler-compiler]]\\n* [[Deterministic parsing]]\\n* [[Generating strings]]\\n* [[Grammar checker]]\\n* [[LALR parser]]\\n* [[Lexical analysis]]\\n* [[Pratt parser]]\\n* [[Shallow parsing]]\\n* [[Left corner parser]]\\n* [[Parsing expression grammar]]\\n* [[ASF+SDF Meta Environment]]\\n* [[DMS Software Reengineering Toolkit]]\\n* [[Program transformation]]\\n* [[Source code generation]]\\n{{colend}}<!--\\nWe don\\'t need a list of every piece of software that parses something.\\nIf you\\'d like to make a SHORT list of NOTABLE applications, feel free.\\n-->\\n\\n== References ==\\n{{reflist}}\\n\\n==Further reading==\\n{{refbegin}}\\n* Chapman, Nigel P., [http://books.google.com/books?id=nEA9AAAAIAAJ&printsec=frontcover \\'\\'LR Parsing: Theory and Practice\\'\\'], [[Cambridge University Press]], 1987. ISBN 0-521-30413-X\\n* Grune, Dick; Jacobs, Ceriel J.H., [http://dickgrune.com/Books/PTAPG_1st_Edition/ \\'\\'Parsing Techniques - A Practical Guide\\'\\'], [[Vrije Universiteit Amsterdam]], Amsterdam, The Netherlands. Originally published by Ellis Horwood, Chichester, England, 1990; ISBN 0-13-651431-6\\n{{refend}}\\n\\n==External links==\\n{{wiktionary|parse}}\\n* [http://www.hwaci.com/sw/lemon/ The Lemon LALR Parser Generator]\\n* [http://nlp.stanford.edu/software/lex-parser.shtml Stanford Parser] The Stanford Parser\\n* [http://www.tule.di.unito.it/ Turin University Parser] Natural language parser for the Italian, open source, developed in Common Lisp by Leonardo Lesmo, University of Torino, Italy.\\n* [http://blogs.perl.org/users/jeffrey_kegler/2014/09/parsing-a-timeline.html Short history of parser construction]\\n\\n[[Category:Algorithms on strings]]\\n[[Category:Compiler construction]]\\n[[Category:Parsing| ]]',\n",
       "     u'contentformat': u'text/x-wiki',\n",
       "     u'contentmodel': u'wikitext'}],\n",
       "   u'title': u'Parsing'}}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsondata['query']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and continue exploring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(jsondata['query'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsondata['query'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the pretty-printed file, we know we are looking for the 'pages' child, which has a page id number. We dont know what this number is, so we cant use it as a key. But instead we can use the 'values()' method on the dictionary to get a list of all the nodes below it. We only need one page, so we take the first of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsondata['query']['pages'].values()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue down the tree, next to the \"revisions\" node. This time, take the *last* revision in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO: write code below to extract the actual content of the article from the last revision in the list, then uncomment and run it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# content = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content is now a text string in Mediawiki's own format. To make sense of it we can use the mwparserfromhell (MWPH for short)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mwparserfromhell as mwph\n",
    "wikicode = mwph.parse(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MWPH supports a variety of methods to explore mediawiki content. The main class is the Wikicode class, which is the type returned by mwph.parse(). e.g. try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikicode.filter_comments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikicode.filter_headings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wikicode.filter_wikilinks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since we want to parse the english text from the article, we want to ignore all these metadata. MWPH has a method to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = wikicode.strip_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is clean enough now that we can save it for parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " saveas(pretty(text), '/home/datascience/labs/lab4/'+title+'.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Stanford Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a terminal window, type\n",
    "<pre>\n",
    "lexparser-gui.sh\n",
    "</pre>\n",
    "\n",
    "This brings up a GUI interface to the Stanford parser. To use it, click on \"Load Parser\" which brings up a file selection dialog. Navigate to\n",
    "\n",
    "<pre>\n",
    "/opt/StanfordParser/stanford-parser-3.4.1-models.jar\n",
    "</pre>\n",
    "\n",
    "and open it.\n",
    "\n",
    "Then you will see a list of parsers to use. Select\n",
    "\n",
    "<pre>\n",
    "englishPCFG.ser.gz\n",
    "</pre>\n",
    "\n",
    "You're now ready to parse some text!\n",
    "\n",
    "Click on the \"Load File\" button, and browse to the lab4 directory and load the parsing.txt file. Click on \"Parse\" to parse the current sentence (highlighted in yellow). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO: look at the very first sentence. Did it parse correctly? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try to analyze some content from Wikipedia. To make our lives simpler, we'll use a simplified english version of wikipedia. Change the URL in the first code box in this file to:\n",
    "<pre>\n",
    "simple.wikipedia.org\n",
    "</pre>\n",
    "and change the query title to 'cat'. Rerun all the cells above. This should produce a file \"cat.txt\" in the lab4 directory. Load that file into the parser, and parse some of the sentences. \n",
    "\n",
    "If you cant access the network, download the file <a href=\"https://bcourses.berkeley.edu/courses/1267848/files/51028369/download?wrap=1\">cat.json</a> into your lab4 directory, and repeat the commands used earlier to load the parsing.json file. \n",
    "\n",
    "We'll now convert the parser output to XML, so we can process it further. Find the script\n",
    "<pre> \n",
    "/opt/StanfordParser/lexparser.sh\n",
    "</pre>\n",
    "and edit it so that its outputFormat is:\n",
    "<pre>\n",
    "-outputFormat \"xmlTree\"\n",
    "</pre>\n",
    "and add a new option:\n",
    "<pre>\n",
    "-outputFormatOptions \"xml\"\n",
    "</pre>\n",
    "save the new script as \n",
    "<pre>\n",
    "parsetoxml.sh\n",
    "</pre>\n",
    "and create an alias to it in your ~/bin directory. Now run from your lab4 directory\n",
    "<pre>\n",
    "parsetoxml.sh cat.txt > cat.xml\n",
    "</pre>\n",
    "you're ready now to analyze the cat data. We'll use Python's builtin ElementTree parser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "parser = etree.XMLParser(recover=True)\n",
    "tree = etree.parse('/home/datascience/labs/lab4/cat.xml',parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the root of this tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root=tree.getroot()\n",
    "root.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root[0].tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. we have found the first sentence. The xmlTree representation is a little tricky however, as POS tags are stored as attributes of nodes rather than node tags. To get to the actual root node, we need to dig a little deeper (and we'll use the second sentence which is a bit more conventional):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "root[1][0][0].attrib['value']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "going down one level gets us to the actual sentence node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s=root[6][0][0][0]\n",
    "s.attrib['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and to get its children we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not too helpful, because the node types are hidden in the value attribs of these nodes. To see them, we can use a python anonymous function and map it over the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map(lambda (x): x.attrib['value'], s[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see if we can find sentences starting with noun phrases containing a given noun. The final function supports a flexible syntax (similar to xpath) for locating elements of given type or attributes. A slash \"/\" is like a directory specifier, and defines a child node. A double slash \"//\" specifies *any* descendent, child, grandchild, great-grandchild etc. The \"node[@value='NP']\" specifies a node with the given attribute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent = s.findall(\"./node[@value='NP']//node[@value='NN']//leaf[@value='cat']\")\n",
    "agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finds all the nodes starting with an 'NP' child of s, and having a 'NN' node above a leaf with 'cat' value. \n",
    "\n",
    "We can similarly look for a verb in a verb phrase under the root node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verb = s.findall(\"./node[@value='VP']//node[@value='VBZ']//leaf[@value='is']\")\n",
    "verb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting these together, we can discover sentences containing a given pair of (agent,action) pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def printnode(node):\n",
    "    for i in node.findall(\".//leaf\"):\n",
    "        print(\" \" + i.attrib['value']),\n",
    "    print('')\n",
    "\n",
    "def testnode(node, agent, action):\n",
    "    aa = node.findall(\"./node[@value='NP']//node[@value='NN']//leaf[@value='\"+agent+\"']\")\n",
    "    bb = node.findall(\"./node[@value='VP']//leaf[@value='\"+action+\"']\")\n",
    "    if (len(aa) > 0 and len(bb) > 0):\n",
    "        printnode(node)    \n",
    "\n",
    "def agentact(node, agent, action):\n",
    "    testnode(node, agent, action)\n",
    "    snodes = node.findall(\".//node[@value='S']\")\n",
    "    for snode in snodes:\n",
    "        testnode(snode, agent, action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agentact(s, title, 'is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can map the agentact function across all the sentences in the Wikipedia entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map(lambda (nn): agentact(nn[0][0][0], title, 'is'), root)\n",
    "[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> TODO: Create a new testnode2 function that captures some more facts about cats. Put it in the cell below, and then evaluate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agentact2(node, agent, action):\n",
    "    testnode2(node, agent, action)\n",
    "    snodes = node.findall(\".//node[@value='S']\")\n",
    "    for snode in snodes:\n",
    "        testnode2(snode, agent, action)\n",
    "        \n",
    "map(lambda (nn): agentact2(nn[0][0][0], title, 'is'), root)\n",
    "[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to fill out the lab responses <a href=\"https://ufl.instructure.com/courses/320501/quizzes/464185\">here</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
